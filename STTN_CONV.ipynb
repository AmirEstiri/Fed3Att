{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STTN-CONV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN72DaNqA2OI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3ea50f-5a6d-4c6c-ab8d-c3062ad5db5e"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDInLdx5oLrc"
      },
      "source": [
        "B = 8\n",
        "H = 10 # Grid height\n",
        "W = 10 # Grid width\n",
        "N = H*W # Nodes in grpah\n",
        "M = 15 # previous time steps\n",
        "T = 5 # future time steps\n",
        "TIME_STEP = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeRJk9ytfaET"
      },
      "source": [
        "def normalize(vec):\n",
        "    return (vec-min(vec))/(max(vec)-min(vec))\n",
        "\n",
        "df = pd.read_csv('gdrive/MyDrive/trace/trace_avg.csv')\n",
        "MAX_COST = 200\n",
        "df = df[df['Cost'] < MAX_COST]\n",
        "df['Cost'] = df['Cost'] / MAX_COST\n",
        "df['Long'] = normalize(df['Long'])\n",
        "df['Lat'] = normalize(df['Lat'])\n",
        "\n",
        "df['TimeStep'] = round(df['TimeSec'] / TIME_STEP)\n",
        "df['LongStep'] = round(df['Long'] * (H-1))\n",
        "df['LatStep'] = round(df['Lat'] * (W-1))\n",
        "df['GraphNode'] = (df['LongStep']-1) * W + df['LatStep']\n",
        "df = df.groupby(['GraphNode', 'TimeStep'])['Cost'].mean().reset_index()\n",
        "\n",
        "############################################################\n",
        "\n",
        "times = np.unique(df['TimeStep'])\n",
        "dataset = []\n",
        "\n",
        "for t in times:\n",
        "    grid = np.ones((H*W, 1)) * -1\n",
        "    df_t = df[df['TimeStep'] == t]\n",
        "    for index, row in df_t.iterrows():\n",
        "        lat_step = row['GraphNode'] % W\n",
        "        long_step = (row['GraphNode'] - lat_step) / W + 1\n",
        "        index = int(W*(long_step-1)+lat_step)\n",
        "        grid[index] = row['Cost']\n",
        "    dataset.append(grid)\n",
        "dataset = np.array(dataset)\n",
        "del df\n",
        "\n",
        "############################################################\n",
        "\n",
        "indf = []\n",
        "outdf = []\n",
        "\n",
        "for i in range(len(dataset)-M-T-1):\n",
        "    indf.append(dataset[i:i+M])\n",
        "    outdf.append(dataset[i+M:i+M+T])\n",
        "\n",
        "del dataset\n",
        "\n",
        "L = len(indf)\n",
        "l = L-L%B\n",
        "indf = np.swapaxes(np.array(indf), 1, 2)[:l]\n",
        "outdf = np.swapaxes(np.array(outdf), 1, 2)[:l]\n",
        "\n",
        "indf = np.reshape(indf, (-1, B, indf.shape[1], indf.shape[2]))\n",
        "outdf = np.reshape(outdf, (-1, B, outdf.shape[1], outdf.shape[2]))\n",
        "\n",
        "l = int(0.9*len(indf))\n",
        "indf_train = indf[:l]\n",
        "outdf_train = outdf[:l]\n",
        "indf_test = indf[l:]\n",
        "outdf_test = outdf[l:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYi7jzBdfgNa"
      },
      "source": [
        "class ConvModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, T):\n",
        "    super(ConvModel, self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(2, 3, padding='same', input_shape=(B,N,M))\n",
        "    self.conv2 = tf.keras.layers.Conv2D(4, 3, padding='same')\n",
        "    self.conv3 = tf.keras.layers.Conv2D(2, 3, padding='same')\n",
        "    self.dense = tf.keras.layers.Dense(T, activation='sigmoid')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = tf.reshape(x, (x.shape[0], x.shape[1], x.shape[2], 1))\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = tf.reshape(x, (x.shape[0], x.shape[1], -1))\n",
        "    x = self.dense(x)\n",
        "    return x\n",
        "\n",
        "def loss_function(real, pred, loss_func):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, -1.0))\n",
        "    if loss_func == 'MSE':\n",
        "        loss = tf.square(tf.cast(real, dtype=tf.float32) - tf.cast(pred, dtype=tf.float32))\n",
        "    elif loss_func == 'MAE':\n",
        "        loss = tf.abs(tf.cast(real, dtype=tf.float32) - tf.cast(pred, dtype=tf.float32))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q13DOmUdsWU7"
      },
      "source": [
        "conv = ConvModel(T)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = conv(inp)\n",
        "        loss = loss_function(tar, pred, 'MSE')\n",
        "\n",
        "    gradients = tape.gradient(loss, conv.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, conv.trainable_variables))\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdmZbIWSqcYm",
        "outputId": "15650943-309b-4c9b-cae9-bf86e87467c2"
      },
      "source": [
        "EPOCHS = 10\n",
        "beginning = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "    for b in range(indf_train.shape[0]):\n",
        "        inp = indf_train[b]\n",
        "        tar = outdf_train[b]\n",
        "\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if b % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {b} Loss {train_loss.result():.4f}')\n",
        "        \n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f}')\n",
        "print(f'Total time: {time.time() - beginning:.2f} secs\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.1268\n",
            "Epoch 1 Batch 10 Loss 0.0478\n",
            "Epoch 1 Batch 20 Loss 0.0298\n",
            "Epoch 1 Batch 30 Loss 0.0282\n",
            "Epoch 1 Batch 40 Loss 0.0229\n",
            "Epoch 1 Batch 50 Loss 0.0235\n",
            "Epoch 1 Batch 60 Loss 0.0206\n",
            "Epoch 1 Batch 70 Loss 0.0207\n",
            "Epoch 1 Batch 80 Loss 0.0211\n",
            "Epoch 1 Batch 90 Loss 0.0199\n",
            "Epoch 1 Batch 100 Loss 0.0200\n",
            "Epoch 1 Batch 110 Loss 0.0187\n",
            "Epoch 1 Batch 120 Loss 0.0176\n",
            "Epoch 1 Batch 130 Loss 0.0167\n",
            "Epoch 1 Loss 0.0166\n",
            "Epoch 2 Batch 0 Loss 0.0165\n",
            "Epoch 2 Batch 10 Loss 0.0158\n",
            "Epoch 2 Batch 20 Loss 0.0149\n",
            "Epoch 2 Batch 30 Loss 0.0149\n",
            "Epoch 2 Batch 40 Loss 0.0143\n",
            "Epoch 2 Batch 50 Loss 0.0146\n",
            "Epoch 2 Batch 60 Loss 0.0141\n",
            "Epoch 2 Batch 70 Loss 0.0146\n",
            "Epoch 2 Batch 80 Loss 0.0147\n",
            "Epoch 2 Batch 90 Loss 0.0143\n",
            "Epoch 2 Batch 100 Loss 0.0145\n",
            "Epoch 2 Batch 110 Loss 0.0141\n",
            "Epoch 2 Batch 120 Loss 0.0137\n",
            "Epoch 2 Batch 130 Loss 0.0134\n",
            "Epoch 2 Loss 0.0133\n",
            "Epoch 3 Batch 0 Loss 0.0133\n",
            "Epoch 3 Batch 10 Loss 0.0130\n",
            "Epoch 3 Batch 20 Loss 0.0127\n",
            "Epoch 3 Batch 30 Loss 0.0126\n",
            "Epoch 3 Batch 40 Loss 0.0124\n",
            "Epoch 3 Batch 50 Loss 0.0127\n",
            "Epoch 3 Batch 60 Loss 0.0124\n",
            "Epoch 3 Batch 70 Loss 0.0128\n",
            "Epoch 3 Batch 80 Loss 0.0129\n",
            "Epoch 3 Batch 90 Loss 0.0127\n",
            "Epoch 3 Batch 100 Loss 0.0129\n",
            "Epoch 3 Batch 110 Loss 0.0127\n",
            "Epoch 3 Batch 120 Loss 0.0125\n",
            "Epoch 3 Batch 130 Loss 0.0123\n",
            "Epoch 3 Loss 0.0122\n",
            "Epoch 4 Batch 0 Loss 0.0122\n",
            "Epoch 4 Batch 10 Loss 0.0121\n",
            "Epoch 4 Batch 20 Loss 0.0119\n",
            "Epoch 4 Batch 30 Loss 0.0119\n",
            "Epoch 4 Batch 40 Loss 0.0117\n",
            "Epoch 4 Batch 50 Loss 0.0119\n",
            "Epoch 4 Batch 60 Loss 0.0118\n",
            "Epoch 4 Batch 70 Loss 0.0120\n",
            "Epoch 4 Batch 80 Loss 0.0121\n",
            "Epoch 4 Batch 90 Loss 0.0120\n",
            "Epoch 4 Batch 100 Loss 0.0122\n",
            "Epoch 4 Batch 110 Loss 0.0120\n",
            "Epoch 4 Batch 120 Loss 0.0119\n",
            "Epoch 4 Batch 130 Loss 0.0118\n",
            "Epoch 4 Loss 0.0117\n",
            "Epoch 5 Batch 0 Loss 0.0117\n",
            "Epoch 5 Batch 10 Loss 0.0116\n",
            "Epoch 5 Batch 20 Loss 0.0115\n",
            "Epoch 5 Batch 30 Loss 0.0115\n",
            "Epoch 5 Batch 40 Loss 0.0114\n",
            "Epoch 5 Batch 50 Loss 0.0115\n",
            "Epoch 5 Batch 60 Loss 0.0113\n",
            "Epoch 5 Batch 70 Loss 0.0115\n",
            "Epoch 5 Batch 80 Loss 0.0116\n",
            "Epoch 5 Batch 90 Loss 0.0115\n",
            "Epoch 5 Batch 100 Loss 0.0117\n",
            "Epoch 5 Batch 110 Loss 0.0116\n",
            "Epoch 5 Batch 120 Loss 0.0115\n",
            "Epoch 5 Batch 130 Loss 0.0114\n",
            "Epoch 5 Loss 0.0114\n",
            "Epoch 6 Batch 0 Loss 0.0114\n",
            "Epoch 6 Batch 10 Loss 0.0113\n",
            "Epoch 6 Batch 20 Loss 0.0112\n",
            "Epoch 6 Batch 30 Loss 0.0112\n",
            "Epoch 6 Batch 40 Loss 0.0111\n",
            "Epoch 6 Batch 50 Loss 0.0111\n",
            "Epoch 6 Batch 60 Loss 0.0110\n",
            "Epoch 6 Batch 70 Loss 0.0112\n",
            "Epoch 6 Batch 80 Loss 0.0113\n",
            "Epoch 6 Batch 90 Loss 0.0112\n",
            "Epoch 6 Batch 100 Loss 0.0114\n",
            "Epoch 6 Batch 110 Loss 0.0113\n",
            "Epoch 6 Batch 120 Loss 0.0112\n",
            "Epoch 6 Batch 130 Loss 0.0111\n",
            "Epoch 6 Loss 0.0111\n",
            "Epoch 7 Batch 0 Loss 0.0111\n",
            "Epoch 7 Batch 10 Loss 0.0111\n",
            "Epoch 7 Batch 20 Loss 0.0110\n",
            "Epoch 7 Batch 30 Loss 0.0110\n",
            "Epoch 7 Batch 40 Loss 0.0109\n",
            "Epoch 7 Batch 50 Loss 0.0109\n",
            "Epoch 7 Batch 60 Loss 0.0108\n",
            "Epoch 7 Batch 70 Loss 0.0110\n",
            "Epoch 7 Batch 80 Loss 0.0110\n",
            "Epoch 7 Batch 90 Loss 0.0110\n",
            "Epoch 7 Batch 100 Loss 0.0111\n",
            "Epoch 7 Batch 110 Loss 0.0110\n",
            "Epoch 7 Batch 120 Loss 0.0110\n",
            "Epoch 7 Batch 130 Loss 0.0109\n",
            "Epoch 7 Loss 0.0109\n",
            "Epoch 8 Batch 0 Loss 0.0109\n",
            "Epoch 8 Batch 10 Loss 0.0109\n",
            "Epoch 8 Batch 20 Loss 0.0108\n",
            "Epoch 8 Batch 30 Loss 0.0108\n",
            "Epoch 8 Batch 40 Loss 0.0107\n",
            "Epoch 8 Batch 50 Loss 0.0108\n",
            "Epoch 8 Batch 60 Loss 0.0107\n",
            "Epoch 8 Batch 70 Loss 0.0108\n",
            "Epoch 8 Batch 80 Loss 0.0109\n",
            "Epoch 8 Batch 90 Loss 0.0108\n",
            "Epoch 8 Batch 100 Loss 0.0109\n",
            "Epoch 8 Batch 110 Loss 0.0109\n",
            "Epoch 8 Batch 120 Loss 0.0108\n",
            "Epoch 8 Batch 130 Loss 0.0108\n",
            "Epoch 8 Loss 0.0108\n",
            "Epoch 9 Batch 0 Loss 0.0108\n",
            "Epoch 9 Batch 10 Loss 0.0107\n",
            "Epoch 9 Batch 20 Loss 0.0106\n",
            "Epoch 9 Batch 30 Loss 0.0107\n",
            "Epoch 9 Batch 40 Loss 0.0106\n",
            "Epoch 9 Batch 50 Loss 0.0106\n",
            "Epoch 9 Batch 60 Loss 0.0106\n",
            "Epoch 9 Batch 70 Loss 0.0107\n",
            "Epoch 9 Batch 80 Loss 0.0107\n",
            "Epoch 9 Batch 90 Loss 0.0107\n",
            "Epoch 9 Batch 100 Loss 0.0107\n",
            "Epoch 9 Batch 110 Loss 0.0107\n",
            "Epoch 9 Batch 120 Loss 0.0107\n",
            "Epoch 9 Batch 130 Loss 0.0106\n",
            "Epoch 9 Loss 0.0106\n",
            "Epoch 10 Batch 0 Loss 0.0106\n",
            "Epoch 10 Batch 10 Loss 0.0106\n",
            "Epoch 10 Batch 20 Loss 0.0105\n",
            "Epoch 10 Batch 30 Loss 0.0105\n",
            "Epoch 10 Batch 40 Loss 0.0105\n",
            "Epoch 10 Batch 50 Loss 0.0105\n",
            "Epoch 10 Batch 60 Loss 0.0105\n",
            "Epoch 10 Batch 70 Loss 0.0106\n",
            "Epoch 10 Batch 80 Loss 0.0106\n",
            "Epoch 10 Batch 90 Loss 0.0106\n",
            "Epoch 10 Batch 100 Loss 0.0106\n",
            "Epoch 10 Batch 110 Loss 0.0106\n",
            "Epoch 10 Batch 120 Loss 0.0106\n",
            "Epoch 10 Batch 130 Loss 0.0105\n",
            "Epoch 10 Loss 0.0105\n",
            "Total time: 2.76 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2blz_X6t7ir",
        "outputId": "d313b809-b1d5-4825-d11f-dd9f58acfe99"
      },
      "source": [
        "print(np.sum([np.prod(v.get_shape().as_list()) for v in conv.trainable_variables]))\n",
        "conv.save_weights('gdrive/MyDrive/trace/conv-1')\n",
        "err = 0.0\n",
        "cnt = 0\n",
        "\n",
        "for b in range(indf_test.shape[0]):\n",
        "    inp = indf_test[b]\n",
        "    tar = outdf_test[b]\n",
        "    pred = conv(inp)\n",
        "    cnt += 1\n",
        "    err += loss_function(tar, pred, 'MAE')\n",
        "\n",
        "print(err/cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "325\n",
            "tf.Tensor(0.056200523, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}